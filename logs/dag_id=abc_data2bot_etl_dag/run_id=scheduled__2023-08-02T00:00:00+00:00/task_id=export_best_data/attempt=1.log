[2023-08-03T07:31:50.801+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T07:31:50.808+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T07:31:50.808+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 3
[2023-08-03T07:31:50.822+0000] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): export_best_data> on 2023-08-02 00:00:00+00:00
[2023-08-03T07:31:50.827+0000] {standard_task_runner.py:57} INFO - Started process 2240 to run task
[2023-08-03T07:31:50.830+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'abc_data2bot_etl_dag', 'export_best_data', 'scheduled__2023-08-02T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/etl_abc_dag.py', '--cfg-path', '/tmp/tmphrockyh_']
[2023-08-03T07:31:50.830+0000] {standard_task_runner.py:85} INFO - Job 8: Subtask export_best_data
[2023-08-03T07:31:50.869+0000] {task_command.py:410} INFO - Running <TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [running]> on host d75389520dc6
[2023-08-03T07:31:50.948+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='idowu' AIRFLOW_CTX_DAG_ID='abc_data2bot_etl_dag' AIRFLOW_CTX_TASK_ID='export_best_data' AIRFLOW_CTX_EXECUTION_DATE='2023-08-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-08-02T00:00:00+00:00'
[2023-08-03T07:31:57.800+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 198, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/export_data.py", line 17, in export_best_performing_data
    best_perfoming_data.to_csv(f, index=False)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/spec.py", line 1569, in close
    self.flush(force=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/spec.py", line 1440, in flush
    if self._upload_chunk(final=force) is not False:
  File "/home/airflow/.local/lib/python3.7/site-packages/s3fs/core.py", line 1985, in _upload_chunk
    self.commit()
  File "/home/airflow/.local/lib/python3.7/site-packages/s3fs/core.py", line 2005, in commit
    **self.kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/s3fs/core.py", line 1852, in _call_s3
    return self.fs.call_s3(method, self.s3_additional_kwargs, *kwarglist, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 88, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 69, in sync
    raise result[0]
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 25, in _runner
    result[0] = await coro
  File "/home/airflow/.local/lib/python3.7/site-packages/s3fs/core.py", line 265, in _call_s3
    raise translate_boto_error(err)
  File "/home/airflow/.local/lib/python3.7/site-packages/s3fs/core.py", line 246, in _call_s3
    out = await method(**additional_kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/aiobotocore/client.py", line 142, in _make_api_call
    operation_model, request_dict, request_context)
  File "/home/airflow/.local/lib/python3.7/site-packages/aiobotocore/client.py", line 161, in _make_request
    return await self._endpoint.make_request(operation_model, request_dict)
  File "/home/airflow/.local/lib/python3.7/site-packages/aiobotocore/endpoint.py", line 77, in _send_request
    request = await self.create_request(request_dict, operation_model)
  File "/home/airflow/.local/lib/python3.7/site-packages/aiobotocore/endpoint.py", line 71, in create_request
    operation_name=operation_model.name)
  File "/home/airflow/.local/lib/python3.7/site-packages/aiobotocore/hooks.py", line 27, in _emit
    response = await handler(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/aiobotocore/signers.py", line 16, in handler
    return await self.sign(operation_name, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/aiobotocore/signers.py", line 63, in sign
    auth.add_auth(request)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/auth.py", line 373, in add_auth
    raise NoCredentialsError()
botocore.exceptions.NoCredentialsError: Unable to locate credentials
[2023-08-03T07:31:57.811+0000] {taskinstance.py:1350} INFO - Marking task as UP_FOR_RETRY. dag_id=abc_data2bot_etl_dag, task_id=export_best_data, execution_date=20230802T000000, start_date=20230803T073150, end_date=20230803T073157
[2023-08-03T07:31:57.824+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 8 for task export_best_data (Unable to locate credentials; 2240)
[2023-08-03T07:31:57.862+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-08-03T07:31:57.883+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-08-03T07:35:12.053+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T07:35:12.060+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T07:35:12.060+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 3
[2023-08-03T07:35:12.072+0000] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): export_best_data> on 2023-08-02 00:00:00+00:00
[2023-08-03T07:35:12.078+0000] {standard_task_runner.py:57} INFO - Started process 4081 to run task
[2023-08-03T07:35:12.080+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'abc_data2bot_etl_dag', 'export_best_data', 'scheduled__2023-08-02T00:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/etl_abc_dag.py', '--cfg-path', '/tmp/tmprc6jm6d9']
[2023-08-03T07:35:12.081+0000] {standard_task_runner.py:85} INFO - Job 17: Subtask export_best_data
[2023-08-03T07:35:12.123+0000] {task_command.py:410} INFO - Running <TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [running]> on host d75389520dc6
[2023-08-03T07:35:12.197+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='idowu' AIRFLOW_CTX_DAG_ID='abc_data2bot_etl_dag' AIRFLOW_CTX_TASK_ID='export_best_data' AIRFLOW_CTX_EXECUTION_DATE='2023-08-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-08-02T00:00:00+00:00'
[2023-08-03T07:35:19.927+0000] {python.py:183} INFO - Done. Returned value was: None
[2023-08-03T07:35:19.934+0000] {taskinstance.py:1350} INFO - Marking task as SUCCESS. dag_id=abc_data2bot_etl_dag, task_id=export_best_data, execution_date=20230802T000000, start_date=20230803T073512, end_date=20230803T073519
[2023-08-03T07:35:19.961+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-08-03T07:35:19.975+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-08-03T07:45:43.349+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T07:45:43.357+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T07:45:43.357+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 3
[2023-08-03T07:45:43.369+0000] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): export_best_data> on 2023-08-02 00:00:00+00:00
[2023-08-03T07:45:43.375+0000] {standard_task_runner.py:57} INFO - Started process 10177 to run task
[2023-08-03T07:45:43.377+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'abc_data2bot_etl_dag', 'export_best_data', 'scheduled__2023-08-02T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/etl_abc_dag.py', '--cfg-path', '/tmp/tmpjovr8o95']
[2023-08-03T07:45:43.378+0000] {standard_task_runner.py:85} INFO - Job 26: Subtask export_best_data
[2023-08-03T07:45:43.419+0000] {task_command.py:410} INFO - Running <TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [running]> on host d75389520dc6
[2023-08-03T07:45:43.489+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='idowu' AIRFLOW_CTX_DAG_ID='abc_data2bot_etl_dag' AIRFLOW_CTX_TASK_ID='export_best_data' AIRFLOW_CTX_EXECUTION_DATE='2023-08-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-08-02T00:00:00+00:00'
[2023-08-03T07:45:55.014+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 198, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/export_data.py", line 28, in export_best_performing_data
    s3.put_object(Bucket=bucket_name, Key=object_key, Body=csv_data, ACL='public-read')
NameError: name 'bucket_name' is not defined
[2023-08-03T07:45:55.020+0000] {taskinstance.py:1350} INFO - Marking task as UP_FOR_RETRY. dag_id=abc_data2bot_etl_dag, task_id=export_best_data, execution_date=20230802T000000, start_date=20230803T074543, end_date=20230803T074555
[2023-08-03T07:45:55.031+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 26 for task export_best_data (name 'bucket_name' is not defined; 10177)
[2023-08-03T07:45:55.068+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-08-03T07:45:55.081+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-08-03T07:51:13.836+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T07:51:13.843+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T07:51:13.843+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 3
[2023-08-03T07:51:13.857+0000] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): export_best_data> on 2023-08-02 00:00:00+00:00
[2023-08-03T07:51:13.863+0000] {standard_task_runner.py:57} INFO - Started process 13419 to run task
[2023-08-03T07:51:13.866+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'abc_data2bot_etl_dag', 'export_best_data', 'scheduled__2023-08-02T00:00:00+00:00', '--job-id', '36', '--raw', '--subdir', 'DAGS_FOLDER/etl_abc_dag.py', '--cfg-path', '/tmp/tmp6izm72iq']
[2023-08-03T07:51:13.866+0000] {standard_task_runner.py:85} INFO - Job 36: Subtask export_best_data
[2023-08-03T07:51:13.907+0000] {task_command.py:410} INFO - Running <TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [running]> on host d75389520dc6
[2023-08-03T07:51:13.976+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='idowu' AIRFLOW_CTX_DAG_ID='abc_data2bot_etl_dag' AIRFLOW_CTX_TASK_ID='export_best_data' AIRFLOW_CTX_EXECUTION_DATE='2023-08-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-08-02T00:00:00+00:00'
[2023-08-03T07:51:24.727+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 198, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/export_data.py", line 30, in export_best_performing_data
    s3.put_object(Bucket=bucket_name, Key=object_key, Body=csv_data, ACL='public-read')
NameError: name 'csv_data' is not defined
[2023-08-03T07:51:24.734+0000] {taskinstance.py:1350} INFO - Marking task as UP_FOR_RETRY. dag_id=abc_data2bot_etl_dag, task_id=export_best_data, execution_date=20230802T000000, start_date=20230803T075113, end_date=20230803T075124
[2023-08-03T07:51:24.752+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 36 for task export_best_data (name 'csv_data' is not defined; 13419)
[2023-08-03T07:51:24.785+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-08-03T07:51:24.798+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-08-03T07:55:01.437+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T07:55:01.444+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T07:55:01.445+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 3
[2023-08-03T07:55:01.457+0000] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): export_best_data> on 2023-08-02 00:00:00+00:00
[2023-08-03T07:55:01.462+0000] {standard_task_runner.py:57} INFO - Started process 15546 to run task
[2023-08-03T07:55:01.464+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'abc_data2bot_etl_dag', 'export_best_data', 'scheduled__2023-08-02T00:00:00+00:00', '--job-id', '42', '--raw', '--subdir', 'DAGS_FOLDER/etl_abc_dag.py', '--cfg-path', '/tmp/tmpqulp25fs']
[2023-08-03T07:55:01.465+0000] {standard_task_runner.py:85} INFO - Job 42: Subtask export_best_data
[2023-08-03T07:55:01.503+0000] {task_command.py:410} INFO - Running <TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [running]> on host d75389520dc6
[2023-08-03T07:55:01.572+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='idowu' AIRFLOW_CTX_DAG_ID='abc_data2bot_etl_dag' AIRFLOW_CTX_TASK_ID='export_best_data' AIRFLOW_CTX_EXECUTION_DATE='2023-08-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-08-02T00:00:00+00:00'
[2023-08-03T07:55:17.038+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 198, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/export_data.py", line 30, in export_best_performing_data
    s3.put_object(Bucket=bucket_name, Key=object_key, Body=best_performing_csv_data, ACL='public-read')
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 386, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 678, in _make_api_call
    api_params, operation_model, context=request_context)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 726, in _convert_to_request_dict
    api_params, operation_model)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/validate.py", line 319, in serialize_to_request
    raise ParamValidationError(report=report.generate_report())
botocore.exceptions.ParamValidationError: Parameter validation failed:
Invalid type for parameter Body, value: None, type: <class 'NoneType'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object
[2023-08-03T07:55:17.045+0000] {taskinstance.py:1350} INFO - Marking task as UP_FOR_RETRY. dag_id=abc_data2bot_etl_dag, task_id=export_best_data, execution_date=20230802T000000, start_date=20230803T075501, end_date=20230803T075517
[2023-08-03T07:55:17.058+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 42 for task export_best_data (Parameter validation failed:
Invalid type for parameter Body, value: None, type: <class 'NoneType'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object; 15546)
[2023-08-03T07:55:17.101+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-08-03T07:55:17.114+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-08-03T08:12:30.547+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T08:12:30.554+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [queued]>
[2023-08-03T08:12:30.554+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 3
[2023-08-03T08:12:30.568+0000] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): export_best_data> on 2023-08-02 00:00:00+00:00
[2023-08-03T08:12:30.574+0000] {standard_task_runner.py:57} INFO - Started process 25539 to run task
[2023-08-03T08:12:30.577+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'abc_data2bot_etl_dag', 'export_best_data', 'scheduled__2023-08-02T00:00:00+00:00', '--job-id', '56', '--raw', '--subdir', 'DAGS_FOLDER/etl_abc_dag.py', '--cfg-path', '/tmp/tmp1r8iq7bd']
[2023-08-03T08:12:30.577+0000] {standard_task_runner.py:85} INFO - Job 56: Subtask export_best_data
[2023-08-03T08:12:30.615+0000] {task_command.py:410} INFO - Running <TaskInstance: abc_data2bot_etl_dag.export_best_data scheduled__2023-08-02T00:00:00+00:00 [running]> on host d75389520dc6
[2023-08-03T08:12:30.687+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='idowu' AIRFLOW_CTX_DAG_ID='abc_data2bot_etl_dag' AIRFLOW_CTX_TASK_ID='export_best_data' AIRFLOW_CTX_EXECUTION_DATE='2023-08-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-08-02T00:00:00+00:00'
[2023-08-03T08:12:39.675+0000] {python.py:183} INFO - Done. Returned value was: None
[2023-08-03T08:12:39.686+0000] {taskinstance.py:1350} INFO - Marking task as SUCCESS. dag_id=abc_data2bot_etl_dag, task_id=export_best_data, execution_date=20230802T000000, start_date=20230803T081230, end_date=20230803T081239
[2023-08-03T08:12:39.736+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-08-03T08:12:39.751+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
